---
layout: post
title: "Week 4"
subtitle: "Week 4 Activities"
background: "/img/bg-post.jpg"
---

# Data Science News
This week I'll be sharing an article by Zackary Nay who compares KDE vs. Normalizing Flows and why MAF wins in higher dimensions.

One of the biggest challenges in high-dimensional density estimation is the curse of dimensionality: as dimensions grow, data becomes sparse, and models like [Kernel Density Estimation (KDE)](https://towardsdatascience.com/kernel-density-estimation-explained-step-by-step-7cc5b5bc4517/) need exponentially more samples to stay accurate. For example, while 22 samples are enough for 1D, over 360,000 are needed in 6D, and over a million in 8D.
KDE works well in low dimensions because it estimates density locally using nearby data points. But as space expands, this local approach breaks down. Bandwidth selection also becomes harder, often requiring computationally heavy methods like leave-one-out cross-validation.
An alternative is Normalizing Flows, specifically [Masked Autoregressive Flows (MAF)](https://towardsdatascience.com/understand-implement-masked-autoregressive-flow-with-tensorflow-9c361cd1354c/). Instead of relying on local neighborhoods, MAF starts with a simple distribution and reshapes it through a series of invertible, differentiable transformations. This global, function-based approach lets MAF generalize without needing to “fill” the space with data points.
In experiments, both KDE and MAF were trained until they achieved a KL divergence of 0.5 across different distributions. Results showed a clear pattern:
<ul>
  <li>Below 5 dimensions, KDE often performed better.</li>

  <li>Above 5 dimensions, MAF vastly outperformed KDE, needing up to 100x fewer samples. For instance, at 7D KDE required over 100,000 samples, while MAF needed just 2,000. MAF also trained about 12x faster.</li>
</ul>
KDE remains an excellent tool for low dimensional problems, but for higher dimensional tasks, MAF is both more data-efficient and computationally effective. If the application involves more than 5 dimensions, MAF is the clear winner.

# Week 4 Reflection
This week, I continued on with trying to determine the estimated replacement costs of each medical imaging asset, which continued to be a challenge. At first, I tried searching up a few assets to see what I could find, and what I'd be working with. Unfortunatey, there was no place that publicly listed exact costs of equipment, and the best I could find was rough estimates for different models and series. This was the main challenge, as I would not be able to get accurate data without breaching confidentiality rules since there does exist a list, but I am not allowed access to it. I was encouraged by my supervisor to utilise ChatGPT to get rough estimates and they understood that I would not be able to get exact figures. Using ChatGPT proved to be much more efficient, as it was able to scrape the internet for websites that had listed rough cost estimates. It was especially useful that ChatGPT also provided the websites it used, as it meant that I could verify its outputs and record the costs down. Through this method, I was able to get a rough estimate of costs for each medical imaging asset. This experience taught me that sometimes it's impossible to obtain highly accurate data, and sometimes you just have to work with what you have within your control. It also taught me that a lot of moving parts are required to get clean, accurate datasets in the industry as each entry can be from multiple different departments within an organisation and if one department is not up to speed, there is potential for lengthy delays. 